defaults:
  - dataset: imdb
  - classifier: lstm
  - logger: local
  - loss: ts_fwd_cross_ent
#  - override hydra/sweeper: basic
#  - override hydra/launcher: basic


teacher:
  depth: ${classifier.num_layers}
  num_components: 1
  use_ckpts: True
  ckpt_init:
    enabled: False
    loc_param: 0.0
  shuffle_ckpts: False
  ckpt_dir:
            "data/experiments/text_classification/classifier_checkpoints/\
            ${classifier.name}${teacher.depth}_${dataset.name}"

trainer:
  num_epochs: 300
  eval_period: 10
  eval_dataset: test
  optimizer:
    _target_: torch.optim.SGD
    lr: 5e-2
    weight_decay: 1e-4
    momentum: 0.9
    nesterov: True
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.num_epochs}
    eta_min: 1e-6
  distill_teacher: True

dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 10
  shuffle: True

distill_loader:
  _target_: gnosis.distillation.dataloaders.DistillLoader
  splits:
    - 0
  temp: 4.0
  batch_size: ${dataloader.batch_size}
  shuffle: True
  drop_last: False
  synth_ratio: 0.0

mixup:
  alpha: 0.

seed: ${trial_id}
trial_id: 0
project_name: gnosis
version: v0.0.11

# Directories for loading and storing data
project_dir: cwd
data_dir: data/experiments/image_classification
log_dir: ${data_dir}/${exp_name}/trial_${trial_id}/${now:%Y-%m-%d_%H-%M-%S}
exp_name:
        "text_${dataset.name}_\
         ts_soft_cross_ent_alpha_${loss.alpha}_temp_${distill_loader.temp}_v0.0.10/\
         lstm${teacher.depth}_${teacher.num_components}"

# Checkpointing
ckpt_store: local
s3_bucket: samuel-stanton-personal-bucket

hydra:
  run:
    dir: ./${log_dir}
  sweep:
    dir: ./${log_dir}
    subdir: .
