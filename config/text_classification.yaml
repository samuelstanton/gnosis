defaults:
  - dataset: imdb
  - classifier: lstm
  - logger: local
  - loss: ts_fwd_cross_ent
#  - override hydra/sweeper: basic
#  - override hydra/launcher: basic


teacher:
  depth: ${classifier.num_layers}
  num_components: 1
  use_ckpts: True
  ckpt_init:
    type: null
    loc_param: 0.0
  shuffle_ckpts: False
  ckpt_dir:
            "data/experiments/text_classification/classifier_checkpoints/\
            ${classifier.name}${teacher.depth}_${dataset.name}/\
            lr${trainer.optimizer.lr}_wd${trainer.optimizer.weight_decay}"

trainer:
  num_epochs: 300
  eval_period: 10
  eval_dataset: test
  optimizer:
    _target_: torch.optim.SGD
    lr: 5e-2
    weight_decay: 1e-4
    momentum: 0.9
    nesterov: True
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.num_epochs}
    eta_min: 1e-6
  distill_teacher: True

dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 10
  shuffle: True

distill_loader:
  _target_: gnosis.distillation.dataloaders.DistillLoader
  splits:
    - 0
  temp: 4.0
  batch_size: ${dataloader.batch_size}
  shuffle: True
  drop_last: False
  synth_ratio: 0.0

mixup:
  alpha: 0.

seed: ${trial_id}
trial_id: 0
project_name: gnosis
version: v0.0.12

# Directories for loading and storing data
project_dir: cwd
data_dir: data/experiments/text_classfication
exp_name:
        "text_${dataset.name}\
         ts_soft_cross_ent_temp_${distill_loader.temp}_v0.0.10/\
         lstm${teacher.depth}_dir${classifier.bidirectional}_${teacher.num_components}/\
         lr${trainer.optimizer.lr}_wd${trainer.optimizer.weight_decay}"
job_name: null
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
log_dir: ${data_dir}/${exp_name}/${job_name}/${timestamp}

# Checkpointing
ckpt_store: local
s3_bucket: samuel-stanton-personal-bucket

hydra:
  run:
    dir: ./${log_dir}
  sweep:
    dir: ./${log_dir}
    subdir: .
