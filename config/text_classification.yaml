defaults:
  - dataset: imdb
  - classifier: lstm
  - logger: local
  - loss: ts_fwd_cross_ent


teacher:
  name: ${classifier.name}
  depth: ${classifier.depth}
  num_components: 1
  use_ckpts: True
  ckpt_init:
    type: null
    loc_param: 0.0
  shuffle_ckpts: False
  ckpt_dir: "test/${teacher.name}${teacher.depth}_${dataset.name}"

trainer:
  num_epochs: 100
  eval_period: 10
  eval_dataset: test
  optimizer:
    _target_: torch.optim.SGD
    lr: 5e-2
    weight_decay: 1e-4
    momentum: 0.9
    nesterov: True
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.num_epochs}
    eta_min: 1e-6
  distill_teacher: True

dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 10
  shuffle: True

distill_loader:
  _target_: gnosis.distillation.dataloaders.DistillLoaderFromLoader
  temp: 1.0

mixup:
  alpha: 0.

seed: ${trial_id}
trial_id: 0
project_name: gnosis
version: v0.1.0

# Directories for loading and storing data
dataset_dir: data/datasets
project_dir: cwd
data_dir: data/experiments/text_classification
exp_name: test/${classifier.name}${classifier.depth}_${dataset.name}
job_name: null
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
log_dir: ${data_dir}/${exp_name}

# Checkpointing
ckpt_store: local
s3_bucket: samuel-stanton-personal-bucket

hydra:
  run:
    dir: ./${log_dir}
  sweep:
    dir: ./${log_dir}
    subdir: .
